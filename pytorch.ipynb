{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input is 28x28x1\n",
    "        # conv1(kernel=5, filters=10) 28x28x10 -> 24x24x10\n",
    "        # max_pool(kernel=2) 24x24x10 -> 12x12x10\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        \n",
    "        # conv2(kernel=5, filters=20) 12x12x10 -> 8x8x20\n",
    "        # max_pool(kernel=2) 8x8x20 -> 4x4x20\n",
    "        x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)), 2))\n",
    "        \n",
    "        # flatten 4x4x20 = 320\n",
    "        x = x.view(-1, 320)\n",
    "        \n",
    "        # 320 -> 50 -> 10\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # transform to logits\n",
    "        return F.log_softmax(x,dim=1)\n",
    "\n",
    "\n",
    "model = SimpleNet()\n",
    "\n",
    "criterion = F.nll_loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:23<00:00, 20.01it/s]\n",
      "  0%|          | 2/468 [00:00<00:25, 18.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "Train set: Average loss: 1.5995, Accuracy: 26749/60000 (44%)\n",
      "Test set: Average loss: 0.4710, Accuracy: 8734/10000 (87%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:25<00:00, 18.72it/s]\n",
      "  0%|          | 2/468 [00:00<00:24, 19.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Train set: Average loss: 0.6339, Accuracy: 48110/60000 (80%)\n",
      "Test set: Average loss: 0.2429, Accuracy: 9268/10000 (92%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:24<00:00, 18.88it/s]\n",
      "  0%|          | 2/468 [00:00<00:25, 18.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Train set: Average loss: 0.4596, Accuracy: 51673/60000 (86%)\n",
      "Test set: Average loss: 0.1680, Accuracy: 9463/10000 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:24<00:00, 13.02it/s]\n",
      "  0%|          | 2/468 [00:00<00:35, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Train set: Average loss: 0.3755, Accuracy: 53168/60000 (88%)\n",
      "Test set: Average loss: 0.1365, Accuracy: 9560/10000 (95%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:23<00:00, 19.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Train set: Average loss: 0.3276, Accuracy: 54169/60000 (90%)\n",
      "Test set: Average loss: 0.1143, Accuracy: 9632/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = 0\n",
    "test_accuracy = 0\n",
    "\n",
    "# train and evaluate\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        data = Variable(images)\n",
    "        target = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(data)\n",
    "        loss = criterion(preds, target)\n",
    "        train_loss += loss.data.item()\n",
    "        pred = preds.data.max(1)[1] # get the index of the max log-probability\n",
    "        train_correct += pred.eq(target.data).cpu().sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100. * train_correct / len(train_loader.dataset)\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval() # set model in inference mode (need this because of dropout)\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for data, target in test_loader:\n",
    "        with torch.no_grad():\n",
    "            data = Variable(data) \n",
    "            target = Variable(target)\n",
    "        \n",
    "        preds = model(data)\n",
    "        test_loss += criterion(preds, target).data.item()\n",
    "        pred = preds.data.max(1)[1] # get the index of the max log-probability\n",
    "        test_correct += pred.eq(target.data).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    test_accuracy = 100. * test_correct / len(test_loader.dataset)\n",
    "       \n",
    "    print(\"\\nEpoch %d\" % epoch)\n",
    "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, train_correct, len(train_loader.dataset),\n",
    "        train_accuracy))\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, test_correct, len(test_loader.dataset),\n",
    "        test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "Please print the training and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 90%\n",
      "Testing Accuracy: 96%\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy: {:.0f}%'.format(train_accuracy))\n",
    "print('Testing Accuracy: {:.0f}%'.format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
